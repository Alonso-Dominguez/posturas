# ğŸ¤– Sistema de Reconocimiento de Gestos en Tiempo Real

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green.svg)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-red.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa un **sistema inteligente de reconocimiento de gestos de mano** que utiliza **Machine Learning** y **Computer Vision** para clasificar y ejecutar acciones basadas en gestos capturados en tiempo real a travÃ©s de la webcam.

### ğŸ¯ Objetivo
Desarrollar una aplicaciÃ³n que pueda reconocer automÃ¡ticamente gestos de mano especÃ­ficos y ejecutar acciones predefinidas, simulando un sistema de control por gestos para aplicaciones multimedia.

### ğŸ”§ TecnologÃ­as Utilizadas
- **Python 3.8+**
- **MediaPipe** - ExtracciÃ³n de landmarks de manos
- **OpenCV** - Procesamiento de video e interfaz visual
- **Scikit-learn** - Modelos de Machine Learning
- **Pandas & NumPy** - ManipulaciÃ³n de datos
- **Matplotlib & Seaborn** - VisualizaciÃ³n de resultados

## ğŸ¯ Gestos Reconocidos

| Gesto | DescripciÃ³n | AcciÃ³n Simulada |
|-------|-------------|-----------------|
| **âœŠ Cerrado** | PuÃ±o cerrado | â¸ï¸ Pausar |
| **âœ‹ Abierto** | Mano completamente abierta | â–¶ï¸ Reproducir |
| **ğŸ‘ Pulgar Arriba** | Like/Me gusta | ğŸ‘ Me Gusta |
| **âœŒï¸ Paz** | SeÃ±al de paz (V) | âœŒï¸ Compartir |
| **ğŸ‘‰ Apuntar** | Dedo Ã­ndice seÃ±alando | ğŸ‘‰ Siguiente |

## ğŸš€ CaracterÃ­sticas Principales

- **Captura de Dataset Personalizada**: Sistema interactivo para generar datos de entrenamiento
- **MÃºltiples Modelos ML**: ComparaciÃ³n entre Logistic Regression y Random Forest
- **Reconocimiento en Tiempo Real**: Procesamiento de video con baja latencia
- **Sistema de Confianza**: Umbrales ajustables para mejorar precisiÃ³n
- **Suavizado de Predicciones**: Buffer temporal para estabilizar resultados
- **Interfaz Visual Intuitiva**: Feedback visual con barras de confianza

## ğŸ“ Estructura del Proyecto

```
gesture-recognition/
â”œâ”€â”€ ğŸ“„ capture_dataset.py      # Captura de datos de entrenamiento
â”œâ”€â”€ ğŸ“„ train_model.py          # Entrenamiento de modelos ML
â”œâ”€â”€ ğŸ“„ real_time_recognition.py # AplicaciÃ³n de reconocimiento
â”œâ”€â”€ ğŸ“„ gesture_dataset.csv     # Dataset generado (despuÃ©s de captura)
â”œâ”€â”€ ğŸ“„ gesture_model.pkl       # Modelo entrenado (despuÃ©s de training)
â”œâ”€â”€ ğŸ“„ gesture_scaler.pkl      # Escalador de datos
â”œâ”€â”€ ğŸ“„ requirements.txt        # Dependencias del proyecto
â””â”€â”€ ğŸ“„ README.md              # Este archivo
```

## ğŸ› ï¸ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el Repositorio
```bash
git clone https://github.com/tu-usuario/gesture-recognition.git
cd gesture-recognition
```

### 2. Crear Entorno Virtual
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar Dependencias
```bash
pip install -r requirements.txt
```

### 4. Verificar Webcam
AsegÃºrate de que tu webcam estÃ© funcionando y no estÃ© siendo utilizada por otras aplicaciones.

## ğŸ“Š Proceso de Desarrollo

### Paso 1: RecolecciÃ³n de Datos ğŸ“¸
```bash
python capture_dataset.py
```

**Funcionalidades:**
- Captura interactiva de gestos a travÃ©s de webcam
- 50 muestras por gesto para dataset balanceado
- ExtracciÃ³n automÃ¡tica de 21 landmarks de mano (42 coordenadas x,y)
- Almacenamiento en formato CSV optimizado

**Controles:**
- `ESPACIO`: Capturar muestra del gesto actual
- `N`: Cambiar al siguiente gesto
- `Q`: Finalizar captura

### Paso 2: Entrenamiento del Modelo ğŸ¤–
```bash
python train_model.py
```

**Proceso automÃ¡tico:**
1. **Carga y validaciÃ³n** del dataset
2. **Preprocesamiento** con StandardScaler
3. **Entrenamiento** de mÃºltiples modelos:
   - Logistic Regression
   - Random Forest Classifier
4. **EvaluaciÃ³n comparativa** con mÃ©tricas de rendimiento
5. **AnÃ¡lisis de umbrales** de confianza
6. **SelecciÃ³n automÃ¡tica** del mejor modelo
7. **Guardado** de modelo y escalador

### Paso 3: Reconocimiento en Tiempo Real ğŸ¥
```bash
python real_time_recognition.py
```

**CaracterÃ­sticas avanzadas:**
- **DetecciÃ³n de manos** con MediaPipe
- **PredicciÃ³n con umbral** de confianza ajustable (90% por defecto)
- **Suavizado temporal** con buffer de 5 frames
- **Feedback visual** en tiempo real
- **Control dinÃ¡mico** de umbrales

**Controles en vivo:**
- `Q`: Salir de la aplicaciÃ³n
- `+`: Aumentar umbral de confianza
- `-`: Disminuir umbral de confianza

## ğŸ“ˆ Resultados y MÃ©tricas

### Rendimiento del Modelo
- **PrecisiÃ³n promedio**: ~95%+
- **Confianza mÃ­nima**: 90% para activar acciones
- **Latencia**: <50ms por frame
- **FPS**: 25-30 en tiempo real

### AnÃ¡lisis de Umbrales
```
Umbral 70%: PrecisiÃ³n=0.891, Cobertura=92%, Rechazadas=8%
Umbral 80%: PrecisiÃ³n=0.923, Cobertura=85%, Rechazadas=15%
Umbral 85%: PrecisiÃ³n=0.941, Cobertura=78%, Rechazadas=22%
Umbral 90%: PrecisiÃ³n=0.967, Cobertura=71%, Rechazadas=29% âœ…
Umbral 95%: PrecisiÃ³n=0.983, Cobertura=54%, Rechazadas=46%
```

## ğŸ® Casos de Uso

### 1. Control de Multimedia
- Control de reproductores de video/audio
- NavegaciÃ³n en presentaciones
- Control de volumen gestual

### 2. Accesibilidad
- Interfaz sin contacto para personas con movilidad limitada
- Control de dispositivos inteligentes
- NavegaciÃ³n web alternativa

### 3. Gaming y Entretenimiento
- Controles gestuales para juegos
- InteracciÃ³n con realidad aumentada
- Aplicaciones educativas interactivas

## ğŸ”¬ Detalles TÃ©cnicos

### ExtracciÃ³n de CaracterÃ­sticas
- **MediaPipe Hands**: 21 landmarks por mano
- **Coordenadas normalizadas**: (x, y) relativas al frame
- **Vector de caracterÃ­sticas**: 42 dimensiones por muestra
- **Preprocesamiento**: StandardScaler para normalizaciÃ³n

### Arquitectura del Modelo
```python
# Mejor configuraciÃ³n encontrada
RandomForestClassifier(
    n_estimators=100,
    random_state=42,
    max_depth=None
)
```

### Pipeline de Procesamiento
1. **Captura de frame** â†’ MediaPipe â†’ **Landmarks**
2. **Landmarks** â†’ StandardScaler â†’ **NormalizaciÃ³n**
3. **Features** â†’ ML Model â†’ **PredicciÃ³n + Confianza**
4. **Buffer temporal** â†’ **Suavizado** â†’ **AcciÃ³n final**

## ğŸš§ Limitaciones Conocidas

- **IluminaciÃ³n**: Sensible a condiciones de luz extremas
- **Fondo**: Mejor rendimiento con fondos contrastantes
- **Distancia**: Ã“ptimo entre 0.5-1.5 metros de la cÃ¡mara
- **Velocidad**: Gestos muy rÃ¡pidos pueden no ser detectados

## ğŸ”® Futuras Mejoras

### TÃ©cnicas
- [ ] Implementar redes neuronales (CNN/LSTM)
- [ ] Reconocimiento de gestos con ambas manos
- [ ] DetecciÃ³n de gestos dinÃ¡micos (movimiento temporal)
- [ ] IntegraciÃ³n con landmarks faciales y corporales

### Funcionalidades
- [ ] Interfaz web con Flask/FastAPI
- [ ] AplicaciÃ³n mÃ³vil con React Native
- [ ] IntegraciÃ³n con APIs de smart home
- [ ] Dashboard de mÃ©tricas en tiempo real

## ğŸ‘¥ Equipo de Desarrollo

| Integrante | Rol | ContribuciÃ³n Principal |
|------------|-----|----------------------|
| **[Tu Nombre]** | Lead Developer | Arquitectura del sistema y ML |
| **[Nombre 2]** | Data Scientist | AnÃ¡lisis de datos y optimizaciÃ³n |
| **[Nombre 3]** | Computer Vision | IntegraciÃ³n MediaPipe y OpenCV |
| **[Nombre 4]** | UI/UX Developer | Interfaz visual y experiencia |

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## ğŸ“ Contacto

- **Email**: tu-email@ejemplo.com
- **LinkedIn**: [Tu Perfil](https://linkedin.com/in/tu-perfil)
- **GitHub**: [Tu Usuario](https://github.com/tu-usuario)

## ğŸ™ Agradecimientos

- **Google MediaPipe Team** por la increÃ­ble biblioteca de ML
- **OpenCV Community** por las herramientas de computer vision
- **Scikit-learn Contributors** por los algoritmos de ML
- **Universidad/InstituciÃ³n** por el apoyo acadÃ©mico

---

â­ **Â¡No olvides dar una estrella al repositorio si te resultÃ³ Ãºtil!** â­