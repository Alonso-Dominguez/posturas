# Sistema de Reconocimiento de Gestos en Tiempo Real

![Python](https://img.shields.io/badge/python-v3.8+-blue.svg)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green.svg)
![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-red.svg)
![License](https://img.shields.io/badge/license-MIT-blue.svg)

### Integrantes:

* Jos√© Alonso Dom√≠nguez Castillo 
* Jos√© David Esquivel Cr√∫z
* Aldo Jes√∫s Martinez Larios

## Descripci√≥n del Proyecto

El fin de este proyecto es implementar un sistema inteligente de reconocimiento de gestos de mano que utiliza Machine Learning y Computer Vision para clasificar y ejecutar acciones basadas en gestos capturados en tiempo real a trav√©s de la webcam.

### Objetivo
El objetivo de este proyecto es el desarrollar una aplicaci√≥n que pueda reconocer autom√°ticamente gestos de mano espec√≠ficos y ejecutar acciones predefinidas, simulando un sistema de control por gestos para aplicaciones multimedia.

### Tecnolog√≠as Utilizadas
- **Python 3.8+**
- **MediaPipe** - Extracci√≥n de landmarks de manos
- **OpenCV** - Procesamiento de video e interfaz visual
- **Scikit-learn** - Modelos de Machine Learning
- **Pandas & NumPy** - Manipulaci√≥n de datos
- **Matplotlib & Seaborn** - Visualizaci√≥n de resultados

## Gestos Reconocidos

| Gesto | Descripci√≥n | Acci√≥n Simulada |
|-------|-------------|-----------------|
| **‚úä Cerrado** | Pu√±o cerrado | ‚è∏ Pausar/Reanudar video |
| **‚úã Abierto** | Mano completamente abierta | üîä Subir volumen |
| **üëç Pulgar Arriba** | Like/Me gusta | üëç Dar "Me gusta" |
| **‚úåÔ∏è Paz** | Se√±al de paz (V) | Adelantar 10 segundos |
| **üëâ Apuntar** | Dedo √≠ndice se√±alando | ‚è≠Ô∏è Siguiente video |


## Caracter√≠sticas Principales

- **Captura de Dataset Personalizada**: Un sistema interactivo que permite generar datos de entrenamiento
- **M√∫ltiples Modelos ML**: Comparaci√≥n entre Logistic Regression y Random Forest
- **Reconocimiento en Tiempo Real**: Un procesamiento de video con baja latencia
- **Sistema de Confianza**: Umbrales ajustables para mejorar precisi√≥n
- **Suavizado de Predicciones**: Cuenta con un buffer temporal para estabilizar resultados
- **Interfaz Visual Intuitiva**: Un feedback visual con barras de confianza

## Estructura del Proyecto

```
gesture-recognition/
‚îú‚îÄ‚îÄ üìÑ capturar_dataset.py      # Captura de datos de entrenamiento
‚îú‚îÄ‚îÄ üìÑ entrenamiento_modelo.py          # Entrenamiento de modelos ML
‚îú‚îÄ‚îÄ üìÑ reconocimiento.py # Aplicaci√≥n de reconocimiento
‚îú‚îÄ‚îÄ üìÑ youtube_gesture_control.py  # üé¨ Control gestual de YouTube
‚îú‚îÄ‚îÄ üìÑ gesture_dataset.csv     # Dataset generado (despu√©s de captura)
‚îú‚îÄ‚îÄ üìÑ gesture_model.pkl       # Modelo entrenado (despu√©s de training)
‚îú‚îÄ‚îÄ üìÑ gesture_scaler.pkl      # Escalador de datos
‚îú‚îÄ‚îÄ üìÑ requirements.txt        # Dependencias del proyecto
‚îî‚îÄ‚îÄ üìÑ README.md              # Este archivo
```

## Instalaci√≥n y Configuraci√≥n

### 1. Clonar el Repositorio
```bash
git clone https://github.com/tu-usuario/gesture-recognition.git
cd gesture-recognition
```

### 2. Crear Entorno Virtual
```bash
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

### 3. Instalar Dependencias
```bash
pip install -r requirements.txt
```

## Proceso de Desarrollo

### Paso 1: Recolecci√≥n de Datos
```bash
python capturar_dataset.py
```

**Funcionalidades:**
- Captura interactiva de gestos a trav√©s de webcam
- 50 muestras por gesto para dataset balanceado
- Extracci√≥n autom√°tica de 21 landmarks de mano (42 coordenadas x,y)
- Almacenamiento en formato CSV optimizado

**Controles:**
- `ESPACIO`: Capturar muestra del gesto actual
- `N`: Cambiar al siguiente gesto
- `Q`: Finalizar captura

### Paso 2: Entrenamiento del Modelo 
```bash
python entrenamiento_modelo.py
```

**Proceso autom√°tico:**
1. **Carga y validaci√≥n** del dataset
2. **Preprocesamiento** con StandardScaler
3. **Entrenamiento** de m√∫ltiples modelos:
   - Logistic Regression
   - Random Forest Classifier
4. **Evaluaci√≥n comparativa** con m√©tricas de rendimiento
5. **An√°lisis de umbrales** de confianza
6. **Selecci√≥n autom√°tica** del mejor modelo
7. **Guardado** de modelo y escalador

### Paso 3: Reconocimiento en Tiempo Real 
```bash
python reconocimiento.py
```

**Caracter√≠sticas avanzadas:**
- **Detecci√≥n de manos** con MediaPipe
- **Predicci√≥n con umbral** de confianza ajustable (90% por defecto)
- **Suavizado temporal** con buffer de 5 frames
- **Feedback visual** en tiempo real
- **Control din√°mico** de umbrales

**Controles en vivo:**
- `Q`: Salir de la aplicaci√≥n
- `+`: Aumentar umbral de confianza
- `-`: Disminuir umbral de confianza


## Limitaciones Conocidas

- **Iluminaci√≥n**: Sensible a condiciones de luz extremas
- **Fondo**: Mejor rendimiento con fondos contrastantes
- **Distancia**: √ìptimo entre 0.5-1.5 metros de la c√°mara
- **Velocidad**: Gestos muy r√°pidos pueden no ser detectados


## Proceso de Implementaci√≥n
Este proyecto fue desarrollado siguiendo una serie de pasos estructurados para lograr un sistema funcional de reconocimiento de gestos por webcam. A continuaci√≥n, se detalla el proceso completo:

**1. Dise√±o del flujo de trabajo**
Se definieron las siguientes etapas fundamentales:

- Captura de datos: obtener muestras de gestos con la webcam.

- Entrenamiento del modelo: procesar las muestras y entrenar un clasificador.

- Reconocimiento en tiempo real: usar el modelo entrenado para detectar gestos en vivo.

**2. Captura de Dataset** (capturar_dataset.py)

Se desarroll√≥ un script que usa MediaPipe para detectar los 21 puntos clave (landmarks) de una mano. Este extrae sus coordenadas (x, y) y las guarda en un archivo CSV y permite capturar gestos de forma interactiva con las teclas:

- ESPACIO: Captura una muestra del gesto actual.

- N: Cambia al siguiente gesto en la lista.

- Q: Termina la sesi√≥n de captura.

Guarda autom√°ticamente los datos etiquetados con el nombre del gesto seleccionado.

**3. Entrenamiento del Modelo** (entrenamiento_modelo.py)

Este script lee el archivo CSV con los datos capturados y escala los datos con StandardScaler para entrena dos modelos de Machine Learning con Scikit-learn (Regresi√≥n Log√≠stica y Random Forest)

Este eval√∫a ambos modelos y selecciona el que obtiene mayor precisi√≥n y guarda el modelo final (modelo.pkl) y el escalador (scaler.pkl) para su uso posterior.

**4. Reconocimiento en Tiempo Real** (reconocimiento.py)

Este script es la aplicaci√≥n principal para uso en vivo. Realiza lo siguiente:

- Captura el video en tiempo real desde la c√°mara.

- Usa MediaPipe para extraer los landmarks.

- Aplica el modelo entrenado para predecir el gesto mostrado.

- Muestra la predicci√≥n, junto con su nivel de confianza, en pantalla.

- Implementa un sistema de suavizado temporal con un buffer de predicciones para evitar falsos positivos.

Acciones disponibles:

* Q: Salir

* + / - : Ajustar el umbral de confianza

## Aplicaci√≥n Pr√°ctica - Control de YouTube

### Control Gestual de YouTube 
```bash
python youtube_gesture_control.py
```

Esta aplicaci√≥n revolucionaria permite controlar YouTube directamente con gestos de mano, creando una experiencia completamente hands-free.

### Demostraci√≥n de Uso

#### **1. Inicio de la Aplicaci√≥n**
Al ejecutar el script, se abre autom√°ticamente la interfaz:

![Interfaz Principal](assets/inicio.png)
*Interfaz principal mostrando la detecci√≥n de manos en tiempo real*

La aplicaci√≥n detecta autom√°ticamente tu mano y muestra:
- **Panel de informaci√≥n** con el gesto actual
- **Barra de confianza** en tiempo real
- **Estado de las acciones** ejecutadas
- **Controles disponibles** en el lateral

#### **2. Ejemplo: Pausar Video con Gesto de Pu√±o**

![Gesto Pausar](assets/pu√±o_cerrado.png)

*Detecci√≥n del gesto "pu√±o cerrado" para pausar el video*

**Proceso paso a paso:**
1. **Detecci√≥n**: La c√°mara detecta tu mano y extrae los 21 landmarks
2. **Clasificaci√≥n**: El modelo ML identifica el gesto como "CERRADO"
3. **Confianza**: Verifica que la confianza sea superior al 90%
4. **Acci√≥n**: Env√≠a autom√°ticamente la tecla `SPACE` a YouTube
5. **Resultado**: El video se pausa instant√°neamente

![Video Pausado](assets/video_pausado.png)
*Video de YouTube pausado exitosamente mediante gesto*

#### **3. Control de Volumen con Mano Abierta**

*Gesto de mano abierta aumentando el volumen*

Cuando detecta una **mano completamente abierta**:
- El sistema reconoce el gesto "ABIERTO"
- Env√≠a la tecla `‚Üë` (flecha arriba)
- YouTube aumenta autom√°ticamente el volumen


### Caracter√≠sticas Especiales

#### **Sistema de Seguridad Inteligente:**
- **Cooldown de 2 segundos**: Previene acciones accidentales repetidas
- **Umbral de confianza**: Solo ejecuta con 90%+ de certeza
- **Control de pausa**: Tecla `P` para pausar/reanudar el control gestual

#### **Feedback Visual Avanzado:**
```
CONTROL GESTUAL DE YOUTUBE
Estado: ACTIVO
Gesto: CERRADO 
Accion: PAUSAR/REANUDAR
Confianza: 94% | Estabilidad: 87%
```

#### **Compatibilidad Universal:**
-  **Chrome, Firefox, Safari, Edge**
-  **YouTube, YouTube Music, YouTube TV**
-  **Windows, macOS, Linux**
-  **Pantalla completa y modo ventana**

### Configuraci√≥n T√©cnica

#### **Requisitos del Sistema:**
- **C√°mara web** funcional (m√≠nimo 720p recomendado)
- **Iluminaci√≥n adecuada** (evitar contraluz)
- **Distancia √≥ptima**: 50cm - 1.5m de la c√°mara
- **RAM**: M√≠nimo 4GB (8GB recomendado)
- **CPU**: Soporte para procesamiento en tiempo real

#### **Instalaci√≥n de Dependencias Adicionales:**
```bash
pip install pyautogui pynput
```

#### **Configuraci√≥n Inicial:**
1. **Abrir YouTube** en tu navegador preferido
2. **Reproducir cualquier video**
3. **Hacer clic en el video** (importante para el foco)
4. **Ejecutar la aplicaci√≥n**
5. **¬°Disfrutar del control gestual!**
